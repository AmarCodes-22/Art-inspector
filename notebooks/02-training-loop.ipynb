{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path(os.path.abspath('')).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.cifar100 import Cifar100\n",
    "from src.models.baseline import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>,\n",
      "            {'aquatic_mammals': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c580>,\n",
      "                                 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c490>},\n",
      "             'fish': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308cee0>,\n",
      "                      'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308cdf0>},\n",
      "             'flowers': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308cb20>,\n",
      "                         'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308ca30>},\n",
      "             'food_containers': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d830384c0>,\n",
      "                                 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d830383d0>},\n",
      "             'fruit_and_vegetables': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c1c0>,\n",
      "                                      'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c0d0>},\n",
      "             'household_electrical_devices': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038880>,\n",
      "                                              'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038790>},\n",
      "             'household_furniture': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d82fbb400>,\n",
      "                                     'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d82fbb310>},\n",
      "             'insects': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c760>,\n",
      "                         'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c670>},\n",
      "             'large_carnivores': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d82fbb220>,\n",
      "                                  'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d82fbb130>},\n",
      "             'large_man-made_outdoor_things': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d830386a0>,\n",
      "                                               'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d830385b0>},\n",
      "             'large_natural_outdoor_scenes': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d82fbb040>,\n",
      "                                              'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038f10>},\n",
      "             'large_omnivores_and_herbivores': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d830382e0>,\n",
      "                                                'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d830381f0>},\n",
      "             'medium_mammals': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038c40>,\n",
      "                                'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038b50>},\n",
      "             'non-insect_invertebrates': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038100>,\n",
      "                                          'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308cfd0>},\n",
      "             'people': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83a6c040>,\n",
      "                        'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1dd431f310>},\n",
      "             'reptiles': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c940>,\n",
      "                          'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c850>},\n",
      "             'small_mammals': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308cd00>,\n",
      "                               'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308cc10>},\n",
      "             'trees': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c3a0>,\n",
      "                       'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d8308c2b0>},\n",
      "             'vehicles_1': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038e20>,\n",
      "                            'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038d30>},\n",
      "             'vehicles_2': {'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038a60>,\n",
      "                            'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1d83038970>}})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# load cifar100\n",
    "def load_cifar100():\n",
    "    cifar100 = Cifar100()\n",
    "    dataloaders = cifar100.load_branch_dataloaders()\n",
    "    return dataloaders\n",
    "\n",
    "cifar_branch_dataloaders = load_cifar100()\n",
    "pprint(cifar_branch_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing model summary\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "Baseline                                           --                        --\n",
      "├─Stem: 1-1                                        [16, 512]                 --\n",
      "│    └─ResNet: 2-1                                 [16, 512]                 --\n",
      "│    │    └─Conv2d: 3-1                            [16, 64, 16, 16]          9,408\n",
      "│    │    └─BatchNorm2d: 3-2                       [16, 64, 16, 16]          128\n",
      "│    │    └─ReLU: 3-3                              [16, 64, 16, 16]          --\n",
      "│    │    └─MaxPool2d: 3-4                         [16, 64, 8, 8]            --\n",
      "│    │    └─Sequential: 3-5                        [16, 64, 8, 8]            73,984\n",
      "│    │    └─Sequential: 3-6                        [16, 128, 4, 4]           230,144\n",
      "│    │    └─Sequential: 3-7                        [16, 256, 2, 2]           919,040\n",
      "│    │    └─Sequential: 3-8                        [16, 512, 1, 1]           3,673,088\n",
      "│    │    └─AdaptiveAvgPool2d: 3-9                 [16, 512, 1, 1]           --\n",
      "│    │    └─Identity: 3-10                         [16, 512]                 --\n",
      "├─Branch: 1-2                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-2                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-11                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-12                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-13                           [16, 5]                   1,285\n",
      "├─Branch: 1-3                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-3                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-14                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-15                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-16                           [16, 5]                   1,285\n",
      "├─Branch: 1-4                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-4                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-17                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-18                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-19                           [16, 5]                   1,285\n",
      "├─Branch: 1-5                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-5                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-20                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-21                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-22                           [16, 5]                   1,285\n",
      "├─Branch: 1-6                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-6                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-23                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-24                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-25                           [16, 5]                   1,285\n",
      "├─Branch: 1-7                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-7                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-26                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-27                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-28                           [16, 5]                   1,285\n",
      "├─Branch: 1-8                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-8                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-29                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-30                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-31                           [16, 5]                   1,285\n",
      "├─Branch: 1-9                                      [16, 5]                   --\n",
      "│    └─Sequential: 2-9                             [16, 5]                   --\n",
      "│    │    └─Linear: 3-32                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-33                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-34                           [16, 5]                   1,285\n",
      "├─Branch: 1-10                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-10                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-35                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-36                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-37                           [16, 5]                   1,285\n",
      "├─Branch: 1-11                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-11                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-38                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-39                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-40                           [16, 5]                   1,285\n",
      "├─Branch: 1-12                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-12                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-41                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-42                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-43                           [16, 5]                   1,285\n",
      "├─Branch: 1-13                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-13                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-44                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-45                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-46                           [16, 5]                   1,285\n",
      "├─Branch: 1-14                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-14                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-47                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-48                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-49                           [16, 5]                   1,285\n",
      "├─Branch: 1-15                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-15                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-50                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-51                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-52                           [16, 5]                   1,285\n",
      "├─Branch: 1-16                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-16                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-53                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-54                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-55                           [16, 5]                   1,285\n",
      "├─Branch: 1-17                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-17                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-56                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-57                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-58                           [16, 5]                   1,285\n",
      "├─Branch: 1-18                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-18                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-59                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-60                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-61                           [16, 5]                   1,285\n",
      "├─Branch: 1-19                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-19                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-62                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-63                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-64                           [16, 5]                   1,285\n",
      "├─Branch: 1-20                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-20                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-65                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-66                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-67                           [16, 5]                   1,285\n",
      "├─Branch: 1-21                                     [16, 5]                   --\n",
      "│    └─Sequential: 2-21                            [16, 5]                   --\n",
      "│    │    └─Linear: 3-68                           [16, 256]                 131,328\n",
      "│    │    └─ReLU: 3-69                             [16, 256]                 --\n",
      "│    │    └─Linear: 3-70                           [16, 5]                   1,285\n",
      "====================================================================================================\n",
      "Total params: 7,558,052\n",
      "Trainable params: 7,558,052\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 332.72\n",
      "====================================================================================================\n",
      "Input size (MB): 0.20\n",
      "Forward/backward pass size (MB): 9.71\n",
      "Params size (MB): 30.23\n",
      "Estimated Total Size (MB): 40.14\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# load baseline model\n",
    "def load_baseline(branch_dataloaders: dict):\n",
    "    resnet_layers = [1,1,1,1]\n",
    "    model = Baseline(stem_layers=resnet_layers)\n",
    "\n",
    "    stem_out_size = model.stem.get_output_size(input_size=(16, 3, 32, 32))\n",
    "\n",
    "    branch_configs = dict()\n",
    "    for branch_name, branch_dataloader in branch_dataloaders.items():\n",
    "        branch_configs[branch_name] = tuple(\n",
    "            (stem_out_size, len(branch_dataloader['train'].dataset.classes))\n",
    "        )\n",
    "\n",
    "    model.initialize_branches(branch_configs=branch_configs)\n",
    "    return model\n",
    "\n",
    "baseline = load_baseline(branch_dataloaders=cifar_branch_dataloaders)\n",
    "baseline.print_model_summary(input_size=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataloaders for branch: people\n",
      "Features shape: torch.Size([16, 3, 32, 32])\n",
      "Labels shape: torch.Size([16])\n",
      "Output shape from single branch: torch.Size([16, 5])\n",
      "Output shape from all branches: torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "# implement a forward pass through the baseline\n",
    "for branch_name, branch_dataloaders in cifar_branch_dataloaders.items():\n",
    "    print(f'Loaded dataloaders for branch: {branch_name}')\n",
    "\n",
    "    train_features, train_labels = next(iter(branch_dataloaders['train']))\n",
    "    \n",
    "    print(f'Features shape: {train_features.shape}')\n",
    "    print(f'Labels shape: {train_labels.shape}')\n",
    "\n",
    "    # get output from a single branch\n",
    "    out = baseline(train_features, branch_name)\n",
    "    print(f'Output shape from single branch: {out.shape}')\n",
    "\n",
    "    # get output from all branches\n",
    "    out = baseline(train_features)\n",
    "    print(f'Output shape from all branches: {out.shape}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 20\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1dd431f310>\n"
     ]
    }
   ],
   "source": [
    "branches = list(cifar_branch_dataloaders.keys())\n",
    "\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "for branch in branches:\n",
    "    train_test_dataloaders = cifar_branch_dataloaders[branch]\n",
    "\n",
    "    train_loader = train_test_dataloaders['train']\n",
    "    test_loader = train_test_dataloaders['test']\n",
    "\n",
    "    # train_loaders.append((branch, train_loader))\n",
    "    # test_loaders.append((branch, test_loader))\n",
    "    train_loaders.append(train_loader)\n",
    "    test_loaders.append(test_loader)\n",
    "\n",
    "print(type(train_loaders), len(train_loaders))\n",
    "print(train_loaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, data in enumerate(zip(*train_loaders)):\n",
    "    for branch_name, branch_batch in zip(branches, data):\n",
    "        print(branch_name)\n",
    "\n",
    "        images, labels = branch_batch\n",
    "        img = images[0].cpu().detach().numpy()\n",
    "        img = np.moveaxis(img, 0, -1)\n",
    "        plt.figure()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        break\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(baseline.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1, Loss: 32.759\n",
      "Epoch: 1, Batch: 2, Loss: 32.187\n",
      "Epoch: 1, Batch: 3, Loss: 32.424\n",
      "Epoch: 1, Batch: 4, Loss: 32.574\n",
      "Epoch: 1, Batch: 5, Loss: 32.489\n",
      "Epoch: 1, Batch: 6, Loss: 32.268\n",
      "Epoch: 1, Batch: 7, Loss: 32.166\n",
      "Epoch: 1, Batch: 8, Loss: 32.555\n",
      "Epoch: 1, Batch: 9, Loss: 32.083\n",
      "Epoch: 1, Batch: 10, Loss: 32.509\n",
      "Epoch: 1, Batch: 11, Loss: 31.825\n",
      "Epoch: 1, Batch: 12, Loss: 31.997\n",
      "Epoch: 1, Batch: 13, Loss: 32.127\n",
      "Epoch: 1, Batch: 14, Loss: 31.508\n",
      "Epoch: 1, Batch: 15, Loss: 31.659\n",
      "Epoch: 1, Batch: 16, Loss: 31.318\n",
      "Epoch: 1, Batch: 17, Loss: 31.442\n",
      "Epoch: 1, Batch: 18, Loss: 31.541\n",
      "Epoch: 1, Batch: 19, Loss: 31.453\n",
      "Epoch: 1, Batch: 20, Loss: 31.335\n",
      "Epoch: 1, Batch: 21, Loss: 31.562\n",
      "Epoch: 1, Batch: 22, Loss: 31.302\n",
      "Epoch: 1, Batch: 23, Loss: 31.177\n",
      "Epoch: 1, Batch: 24, Loss: 31.266\n",
      "Epoch: 1, Batch: 25, Loss: 31.450\n",
      "Epoch: 1, Batch: 26, Loss: 30.898\n",
      "Epoch: 1, Batch: 27, Loss: 30.972\n",
      "Epoch: 1, Batch: 28, Loss: 31.045\n",
      "Epoch: 1, Batch: 29, Loss: 31.159\n",
      "Epoch: 1, Batch: 30, Loss: 30.893\n",
      "Epoch: 1, Batch: 31, Loss: 31.516\n",
      "Epoch: 1, Batch: 32, Loss: 30.989\n",
      "Epoch: 1, Batch: 33, Loss: 31.348\n",
      "Epoch: 1, Batch: 34, Loss: 30.953\n",
      "Epoch: 1, Batch: 35, Loss: 30.684\n",
      "Epoch: 1, Batch: 36, Loss: 30.322\n",
      "Epoch: 1, Batch: 37, Loss: 30.628\n",
      "Epoch: 1, Batch: 38, Loss: 30.623\n",
      "Epoch: 1, Batch: 39, Loss: 30.405\n",
      "Epoch: 1, Batch: 40, Loss: 30.754\n",
      "Epoch: 1, Batch: 41, Loss: 30.544\n",
      "Epoch: 1, Batch: 42, Loss: 30.712\n",
      "Epoch: 1, Batch: 43, Loss: 30.378\n",
      "Epoch: 1, Batch: 44, Loss: 31.157\n",
      "Epoch: 1, Batch: 45, Loss: 29.926\n",
      "Epoch: 1, Batch: 46, Loss: 30.083\n",
      "Epoch: 1, Batch: 47, Loss: 29.988\n",
      "Epoch: 1, Batch: 48, Loss: 30.256\n",
      "Epoch: 1, Batch: 49, Loss: 30.472\n",
      "Epoch: 1, Batch: 50, Loss: 30.424\n",
      "Epoch: 1, Batch: 51, Loss: 30.149\n",
      "Epoch: 1, Batch: 52, Loss: 29.963\n",
      "Epoch: 1, Batch: 53, Loss: 29.832\n",
      "Epoch: 1, Batch: 54, Loss: 30.152\n",
      "Epoch: 1, Batch: 55, Loss: 29.719\n",
      "Epoch: 1, Batch: 56, Loss: 30.116\n",
      "Epoch: 1, Batch: 57, Loss: 30.071\n",
      "Epoch: 1, Batch: 58, Loss: 29.986\n",
      "Epoch: 1, Batch: 59, Loss: 29.645\n",
      "Epoch: 1, Batch: 60, Loss: 29.519\n",
      "Epoch: 1, Batch: 61, Loss: 29.682\n",
      "Epoch: 1, Batch: 62, Loss: 29.863\n",
      "Epoch: 1, Batch: 63, Loss: 29.360\n",
      "Epoch: 1, Batch: 64, Loss: 30.010\n",
      "Epoch: 1, Batch: 65, Loss: 28.958\n",
      "Epoch: 1, Batch: 66, Loss: 29.680\n",
      "Epoch: 1, Batch: 67, Loss: 29.465\n",
      "Epoch: 1, Batch: 68, Loss: 29.134\n",
      "Epoch: 1, Batch: 69, Loss: 29.526\n",
      "Epoch: 1, Batch: 70, Loss: 28.744\n",
      "Epoch: 1, Batch: 71, Loss: 29.221\n",
      "Epoch: 1, Batch: 72, Loss: 29.361\n",
      "Epoch: 1, Batch: 73, Loss: 28.975\n",
      "Epoch: 1, Batch: 74, Loss: 29.622\n",
      "Epoch: 1, Batch: 75, Loss: 28.496\n",
      "Epoch: 1, Batch: 76, Loss: 29.320\n",
      "Epoch: 1, Batch: 77, Loss: 28.803\n",
      "Epoch: 1, Batch: 78, Loss: 28.467\n",
      "Epoch: 1, Batch: 79, Loss: 28.894\n",
      "Epoch: 1, Batch: 80, Loss: 28.977\n",
      "Epoch: 1, Batch: 81, Loss: 28.635\n",
      "Epoch: 1, Batch: 82, Loss: 28.059\n",
      "Epoch: 1, Batch: 83, Loss: 28.507\n",
      "Epoch: 1, Batch: 84, Loss: 28.262\n",
      "Epoch: 1, Batch: 85, Loss: 28.418\n",
      "Epoch: 1, Batch: 86, Loss: 29.210\n",
      "Epoch: 1, Batch: 87, Loss: 28.751\n",
      "Epoch: 1, Batch: 88, Loss: 29.404\n",
      "Epoch: 1, Batch: 89, Loss: 28.860\n",
      "Epoch: 1, Batch: 90, Loss: 28.113\n",
      "Epoch: 1, Batch: 91, Loss: 29.236\n",
      "Epoch: 1, Batch: 92, Loss: 29.074\n",
      "Epoch: 1, Batch: 93, Loss: 28.092\n",
      "Epoch: 1, Batch: 94, Loss: 27.684\n",
      "Epoch: 1, Batch: 95, Loss: 28.512\n",
      "Epoch: 1, Batch: 96, Loss: 28.626\n",
      "Epoch: 1, Batch: 97, Loss: 28.464\n",
      "Epoch: 1, Batch: 98, Loss: 28.071\n",
      "Epoch: 1, Batch: 99, Loss: 27.792\n",
      "Epoch: 1, Batch: 100, Loss: 27.828\n",
      "Epoch: 1, Batch: 101, Loss: 28.081\n",
      "Epoch: 1, Batch: 102, Loss: 28.912\n",
      "Epoch: 1, Batch: 103, Loss: 28.637\n",
      "Epoch: 1, Batch: 104, Loss: 27.771\n",
      "Epoch: 1, Batch: 105, Loss: 27.891\n",
      "Epoch: 1, Batch: 106, Loss: 27.508\n",
      "Epoch: 1, Batch: 107, Loss: 28.284\n",
      "Epoch: 1, Batch: 108, Loss: 28.025\n",
      "Epoch: 1, Batch: 109, Loss: 27.470\n",
      "Epoch: 1, Batch: 110, Loss: 27.485\n",
      "Epoch: 1, Batch: 111, Loss: 27.211\n",
      "Epoch: 1, Batch: 112, Loss: 27.245\n",
      "Epoch: 1, Batch: 113, Loss: 28.017\n",
      "Epoch: 1, Batch: 114, Loss: 26.780\n",
      "Epoch: 1, Batch: 115, Loss: 27.179\n",
      "Epoch: 1, Batch: 116, Loss: 27.908\n",
      "Epoch: 1, Batch: 117, Loss: 28.178\n",
      "Epoch: 1, Batch: 118, Loss: 27.807\n",
      "Epoch: 1, Batch: 119, Loss: 27.439\n",
      "Epoch: 1, Batch: 120, Loss: 27.975\n",
      "Epoch: 1, Batch: 121, Loss: 27.097\n",
      "Epoch: 1, Batch: 122, Loss: 27.541\n",
      "Epoch: 1, Batch: 123, Loss: 26.510\n",
      "Epoch: 1, Batch: 124, Loss: 28.160\n",
      "Epoch: 1, Batch: 125, Loss: 27.803\n",
      "Epoch: 1, Batch: 126, Loss: 27.392\n",
      "Epoch: 1, Batch: 127, Loss: 28.075\n",
      "Epoch: 1, Batch: 128, Loss: 27.874\n",
      "Epoch: 1, Batch: 129, Loss: 27.700\n",
      "Epoch: 1, Batch: 130, Loss: 26.900\n",
      "Epoch: 1, Batch: 131, Loss: 26.413\n",
      "Epoch: 1, Batch: 132, Loss: 27.584\n",
      "Epoch: 1, Batch: 133, Loss: 27.274\n",
      "Epoch: 1, Batch: 134, Loss: 27.432\n",
      "Epoch: 1, Batch: 135, Loss: 26.522\n",
      "Epoch: 1, Batch: 136, Loss: 27.495\n",
      "Epoch: 1, Batch: 137, Loss: 27.537\n",
      "Epoch: 1, Batch: 138, Loss: 27.173\n",
      "Epoch: 1, Batch: 139, Loss: 26.940\n",
      "Epoch: 1, Batch: 140, Loss: 28.718\n",
      "Epoch: 1, Batch: 141, Loss: 26.106\n",
      "Epoch: 1, Batch: 142, Loss: 27.132\n",
      "Epoch: 1, Batch: 143, Loss: 26.923\n",
      "Epoch: 1, Batch: 144, Loss: 27.242\n",
      "Epoch: 1, Batch: 145, Loss: 26.809\n",
      "Epoch: 1, Batch: 146, Loss: 26.416\n",
      "Epoch: 1, Batch: 147, Loss: 26.785\n",
      "Epoch: 1, Batch: 148, Loss: 26.182\n",
      "Epoch: 1, Batch: 149, Loss: 26.680\n",
      "Epoch: 1, Batch: 150, Loss: 26.056\n",
      "Epoch: 1, Batch: 151, Loss: 26.039\n",
      "Epoch: 1, Batch: 152, Loss: 25.567\n",
      "Epoch: 1, Batch: 153, Loss: 27.310\n",
      "Epoch: 1, Batch: 154, Loss: 26.247\n",
      "Epoch: 1, Batch: 155, Loss: 27.517\n",
      "Epoch: 1, Batch: 156, Loss: 26.626\n",
      "Epoch: 1, Batch: 157, Loss: 26.906\n",
      "Epoch: 2, Batch: 1, Loss: 26.746\n",
      "Epoch: 2, Batch: 2, Loss: 26.995\n",
      "Epoch: 2, Batch: 3, Loss: 25.587\n",
      "Epoch: 2, Batch: 4, Loss: 25.797\n",
      "Epoch: 2, Batch: 5, Loss: 26.285\n",
      "Epoch: 2, Batch: 6, Loss: 26.644\n",
      "Epoch: 2, Batch: 7, Loss: 26.668\n",
      "Epoch: 2, Batch: 8, Loss: 26.346\n",
      "Epoch: 2, Batch: 9, Loss: 25.467\n",
      "Epoch: 2, Batch: 10, Loss: 26.692\n",
      "Epoch: 2, Batch: 11, Loss: 25.844\n",
      "Epoch: 2, Batch: 12, Loss: 26.490\n",
      "Epoch: 2, Batch: 13, Loss: 25.831\n",
      "Epoch: 2, Batch: 14, Loss: 26.342\n",
      "Epoch: 2, Batch: 15, Loss: 26.548\n",
      "Epoch: 2, Batch: 16, Loss: 24.962\n",
      "Epoch: 2, Batch: 17, Loss: 25.508\n",
      "Epoch: 2, Batch: 18, Loss: 25.886\n",
      "Epoch: 2, Batch: 19, Loss: 25.737\n",
      "Epoch: 2, Batch: 20, Loss: 25.347\n",
      "Epoch: 2, Batch: 21, Loss: 24.097\n",
      "Epoch: 2, Batch: 22, Loss: 25.579\n",
      "Epoch: 2, Batch: 23, Loss: 24.407\n",
      "Epoch: 2, Batch: 24, Loss: 26.165\n",
      "Epoch: 2, Batch: 25, Loss: 25.953\n",
      "Epoch: 2, Batch: 26, Loss: 25.139\n",
      "Epoch: 2, Batch: 27, Loss: 26.552\n",
      "Epoch: 2, Batch: 28, Loss: 24.397\n",
      "Epoch: 2, Batch: 29, Loss: 24.853\n",
      "Epoch: 2, Batch: 30, Loss: 26.601\n",
      "Epoch: 2, Batch: 31, Loss: 26.115\n",
      "Epoch: 2, Batch: 32, Loss: 25.577\n",
      "Epoch: 2, Batch: 33, Loss: 26.510\n",
      "Epoch: 2, Batch: 34, Loss: 24.677\n",
      "Epoch: 2, Batch: 35, Loss: 25.398\n",
      "Epoch: 2, Batch: 36, Loss: 26.175\n",
      "Epoch: 2, Batch: 37, Loss: 25.933\n",
      "Epoch: 2, Batch: 38, Loss: 26.117\n",
      "Epoch: 2, Batch: 39, Loss: 23.828\n",
      "Epoch: 2, Batch: 40, Loss: 24.626\n",
      "Epoch: 2, Batch: 41, Loss: 25.203\n",
      "Epoch: 2, Batch: 42, Loss: 24.910\n",
      "Epoch: 2, Batch: 43, Loss: 25.369\n",
      "Epoch: 2, Batch: 44, Loss: 24.965\n",
      "Epoch: 2, Batch: 45, Loss: 24.572\n",
      "Epoch: 2, Batch: 46, Loss: 26.015\n",
      "Epoch: 2, Batch: 47, Loss: 26.047\n",
      "Epoch: 2, Batch: 48, Loss: 25.366\n",
      "Epoch: 2, Batch: 49, Loss: 25.580\n",
      "Epoch: 2, Batch: 50, Loss: 25.189\n",
      "Epoch: 2, Batch: 51, Loss: 24.341\n",
      "Epoch: 2, Batch: 52, Loss: 25.907\n",
      "Epoch: 2, Batch: 53, Loss: 26.793\n",
      "Epoch: 2, Batch: 54, Loss: 24.352\n",
      "Epoch: 2, Batch: 55, Loss: 25.645\n",
      "Epoch: 2, Batch: 56, Loss: 25.027\n",
      "Epoch: 2, Batch: 57, Loss: 25.029\n",
      "Epoch: 2, Batch: 58, Loss: 24.803\n",
      "Epoch: 2, Batch: 59, Loss: 25.763\n",
      "Epoch: 2, Batch: 60, Loss: 25.237\n",
      "Epoch: 2, Batch: 61, Loss: 26.026\n",
      "Epoch: 2, Batch: 62, Loss: 24.448\n",
      "Epoch: 2, Batch: 63, Loss: 25.926\n",
      "Epoch: 2, Batch: 64, Loss: 24.772\n",
      "Epoch: 2, Batch: 65, Loss: 25.187\n",
      "Epoch: 2, Batch: 66, Loss: 23.932\n",
      "Epoch: 2, Batch: 67, Loss: 25.073\n",
      "Epoch: 2, Batch: 68, Loss: 25.442\n",
      "Epoch: 2, Batch: 69, Loss: 25.832\n",
      "Epoch: 2, Batch: 70, Loss: 25.599\n",
      "Epoch: 2, Batch: 71, Loss: 24.977\n",
      "Epoch: 2, Batch: 72, Loss: 25.270\n",
      "Epoch: 2, Batch: 73, Loss: 26.275\n",
      "Epoch: 2, Batch: 74, Loss: 24.747\n",
      "Epoch: 2, Batch: 75, Loss: 25.441\n",
      "Epoch: 2, Batch: 76, Loss: 26.357\n",
      "Epoch: 2, Batch: 77, Loss: 24.808\n",
      "Epoch: 2, Batch: 78, Loss: 24.563\n",
      "Epoch: 2, Batch: 79, Loss: 24.909\n",
      "Epoch: 2, Batch: 80, Loss: 25.285\n",
      "Epoch: 2, Batch: 81, Loss: 24.540\n",
      "Epoch: 2, Batch: 82, Loss: 23.950\n",
      "Epoch: 2, Batch: 83, Loss: 26.461\n",
      "Epoch: 2, Batch: 84, Loss: 25.451\n",
      "Epoch: 2, Batch: 85, Loss: 23.400\n",
      "Epoch: 2, Batch: 86, Loss: 24.329\n",
      "Epoch: 2, Batch: 87, Loss: 24.909\n",
      "Epoch: 2, Batch: 88, Loss: 24.136\n",
      "Epoch: 2, Batch: 89, Loss: 25.082\n",
      "Epoch: 2, Batch: 90, Loss: 25.045\n",
      "Epoch: 2, Batch: 91, Loss: 24.263\n",
      "Epoch: 2, Batch: 92, Loss: 24.292\n",
      "Epoch: 2, Batch: 93, Loss: 24.576\n",
      "Epoch: 2, Batch: 94, Loss: 24.547\n",
      "Epoch: 2, Batch: 95, Loss: 26.176\n",
      "Epoch: 2, Batch: 96, Loss: 24.464\n",
      "Epoch: 2, Batch: 97, Loss: 25.221\n",
      "Epoch: 2, Batch: 98, Loss: 25.050\n",
      "Epoch: 2, Batch: 99, Loss: 25.798\n",
      "Epoch: 2, Batch: 100, Loss: 25.701\n",
      "Epoch: 2, Batch: 101, Loss: 24.435\n",
      "Epoch: 2, Batch: 102, Loss: 23.394\n",
      "Epoch: 2, Batch: 103, Loss: 25.732\n",
      "Epoch: 2, Batch: 104, Loss: 24.010\n",
      "Epoch: 2, Batch: 105, Loss: 24.373\n",
      "Epoch: 2, Batch: 106, Loss: 24.450\n",
      "Epoch: 2, Batch: 107, Loss: 25.024\n",
      "Epoch: 2, Batch: 108, Loss: 25.527\n",
      "Epoch: 2, Batch: 109, Loss: 24.340\n",
      "Epoch: 2, Batch: 110, Loss: 25.260\n",
      "Epoch: 2, Batch: 111, Loss: 25.085\n",
      "Epoch: 2, Batch: 112, Loss: 24.149\n",
      "Epoch: 2, Batch: 113, Loss: 24.355\n",
      "Epoch: 2, Batch: 114, Loss: 24.853\n",
      "Epoch: 2, Batch: 115, Loss: 24.063\n",
      "Epoch: 2, Batch: 116, Loss: 25.507\n",
      "Epoch: 2, Batch: 117, Loss: 24.096\n",
      "Epoch: 2, Batch: 118, Loss: 24.627\n",
      "Epoch: 2, Batch: 119, Loss: 25.677\n",
      "Epoch: 2, Batch: 120, Loss: 24.634\n",
      "Epoch: 2, Batch: 121, Loss: 24.296\n",
      "Epoch: 2, Batch: 122, Loss: 23.656\n",
      "Epoch: 2, Batch: 123, Loss: 24.313\n",
      "Epoch: 2, Batch: 124, Loss: 24.721\n",
      "Epoch: 2, Batch: 125, Loss: 25.035\n",
      "Epoch: 2, Batch: 126, Loss: 23.684\n",
      "Epoch: 2, Batch: 127, Loss: 24.106\n",
      "Epoch: 2, Batch: 128, Loss: 25.217\n",
      "Epoch: 2, Batch: 129, Loss: 23.744\n",
      "Epoch: 2, Batch: 130, Loss: 24.811\n",
      "Epoch: 2, Batch: 131, Loss: 24.293\n",
      "Epoch: 2, Batch: 132, Loss: 24.806\n",
      "Epoch: 2, Batch: 133, Loss: 22.920\n",
      "Epoch: 2, Batch: 134, Loss: 24.690\n",
      "Epoch: 2, Batch: 135, Loss: 24.042\n",
      "Epoch: 2, Batch: 136, Loss: 23.607\n",
      "Epoch: 2, Batch: 137, Loss: 24.754\n",
      "Epoch: 2, Batch: 138, Loss: 23.281\n",
      "Epoch: 2, Batch: 139, Loss: 24.780\n",
      "Epoch: 2, Batch: 140, Loss: 24.048\n",
      "Epoch: 2, Batch: 141, Loss: 24.292\n",
      "Epoch: 2, Batch: 142, Loss: 23.551\n",
      "Epoch: 2, Batch: 143, Loss: 24.134\n",
      "Epoch: 2, Batch: 144, Loss: 24.588\n",
      "Epoch: 2, Batch: 145, Loss: 24.659\n",
      "Epoch: 2, Batch: 146, Loss: 24.306\n",
      "Epoch: 2, Batch: 147, Loss: 25.356\n",
      "Epoch: 2, Batch: 148, Loss: 24.884\n",
      "Epoch: 2, Batch: 149, Loss: 23.886\n",
      "Epoch: 2, Batch: 150, Loss: 24.830\n",
      "Epoch: 2, Batch: 151, Loss: 24.425\n",
      "Epoch: 2, Batch: 152, Loss: 24.008\n",
      "Epoch: 2, Batch: 153, Loss: 23.984\n",
      "Epoch: 2, Batch: 154, Loss: 23.626\n",
      "Epoch: 2, Batch: 155, Loss: 24.345\n",
      "Epoch: 2, Batch: 156, Loss: 24.492\n",
      "Epoch: 2, Batch: 157, Loss: 26.268\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "\n",
    "    for i, data in enumerate(zip(*train_loaders)):\n",
    "        running_loss = 0.0\n",
    "        for branch_num, (branch_name, branch_batch) in enumerate(zip(branches, data)):\n",
    "\n",
    "            images, labels = branch_batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = baseline(images, branch_name)\n",
    "\n",
    "            # backward\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss:.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(baseline.state_dict(), os.path.join(Path(os.path.abspath('')).parent, 'models', 'baseline_trained_2_epochs_cifar100.pth'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8cd7c2e683202aa99c6bd22cb91ca768a1d33a054ff7cab7f64305a39e1a247d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
